{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.2-LABSPREAD",
      "provenance": [],
      "collapsed_sections": [
        "6RcfmRBjMzcW"
      ],
      "authorship_tag": "ABX9TyP3mWQtcgZzzyt9UO2ralCp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irenetsk/mastersthesis/blob/main/1_2_LABSPREAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/My Drive/\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.semi_supervised import LabelPropagation\n",
        "import random\n",
        "! pip install --upgrade tbb\n",
        "! pip install umap-learn\n",
        "import umap\n",
        "! pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "zoz315Ga4_11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c39c662-5402-4610-9988-e0cdb52c6a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n",
            "Collecting tbb\n",
            "  Downloading tbb-2021.5.1-py2.py3-none-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 11.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: tbb\n",
            "Successfully installed tbb-2021.5.1\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 38.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.64.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=afb3348f513810439f27e97b2c97250a51e8a64883b27f1b7643e1d83b65239b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=681d9b8127e9d9f5d760ab283131e449cadfae90bae3b7a3c1c0a6b8aba5cab9\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.6 umap-learn-0.5.3\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 34.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 48.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 16.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=365a97b38d5bf7840bf831baff41dc4a7861673906e0b0154ddb151e3aa36711\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-grained (4 classes)"
      ],
      "metadata": {
        "id": "_0-J2IrjkcST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset & SBERT\n",
        "1 = Enthusiastic / 2 = Neutral / 3 = Sad / 4 = Angry\n"
      ],
      "metadata": {
        "id": "6RcfmRBjMzcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# READSPEAKER SENTENCES\n",
        "angrysents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_02_anger.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    angrysents.append(sent)\n",
        "# angrysents = list(set(angrysents))                                # to ensure the deletion of duplicates\n",
        "angrylabels = ['4' for i in range(len(angrysents))]\n",
        "\n",
        "enthusents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_03_happy.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    enthusents.append(sent)\n",
        "# enthusents = list(set(enthusents))\n",
        "enthulabels = ['1' for i in range(len(enthusents))]\n",
        "\n",
        "sadsents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_04_sadness.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    sadsents.append(sent)\n",
        "# sadsents = list(set(sadsents))\n",
        "sadlabels = ['3' for i in range(len(sadsents))]\n",
        "\n",
        "# MY ANNOTATIONS\n",
        "trainsents = []\n",
        "with open(\"./thesis/myannotations/sents.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    trainsents.append(sent)\n",
        "\n",
        "trainlabels = []\n",
        "with open(\"./thesis/myannotations/labels.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for label in f:\n",
        "    trainlabels.append(label)\n",
        "\n",
        "with open(\"./thesis/myannotations/moreneutral_sents.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    trainsents.append(sent)\n",
        "  for i in range(len(f)):\n",
        "    trainlabels.append('2')\n",
        "\n",
        "rs_sents = angrysents + enthusents + sadsents\n",
        "rs_labels = angrylabels + enthulabels + sadlabels\n",
        "combinedsents = trainsents + rs_sents\n",
        "combinedlabels = trainlabels + rs_labels"
      ],
      "metadata": {
        "id": "k3OtDSvmvnsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'enthusiastic: \\t{len(enthusents)} \\nsad: \\t\\t{len(sadsents)}\\nangry: \\t\\t{len(angrysents)}\\nmy annotations: {len(trainsents)}\\ntotal: \\t\\t{sum([len(angrysents), len(enthusents), len(sadsents), len(trainsents)])}')"
      ],
      "metadata": {
        "id": "sARtP_0LOG7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d783012c-e8e1-4137-af3b-3e07d75d62d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enthusiastic: \t650 \n",
            "sad: \t\t638\n",
            "angry: \t\t601\n",
            "my annotations: 704\n",
            "total: \t\t2593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sent2idx = dict()\n",
        "X_lab, X_unlab = [],[]\n",
        "y_lab, y_unlab = [],[]\n",
        "\n",
        "with open(\"thesis/corpora/eng-simple_wikipedia_2021_10K-sentences.txt\", encoding='utf-8') as f1:\n",
        "  f1 = (f1.read()).split(\"\\n\")\n",
        "  with open(\"thesis/corpora/eng-uk_web-public_2018_10K-sentences.txt\", encoding='utf-8') as f2:\n",
        "    f2 = (f2.read()).split(\"\\n\")\n",
        "    for i, line in enumerate(f1+f2):\n",
        "        sent = (line[(len(str(i+1))):]).strip()                                     # removes the line number and the tab char\n",
        "        if sent in trainsents:\n",
        "          # sent2idx[sent] = i                                                      # to find them in the embedding\n",
        "          X_lab.append(sent)\n",
        "          y_lab.append(int(combinedlabels[combinedsents.index(sent)]))\n",
        "        else:\n",
        "          X_unlab.append(sent)\n",
        "          y_unlab.append(-1)\n",
        "\n",
        "# track = len(unlabeled+labeled)\n",
        "for i, sent in enumerate(rs_sents):\n",
        "  X_lab.append(sent)\n",
        "  y_lab.append(int(combinedlabels[combinedsents.index(sent)]))\n",
        "  # sent2idx[sent] = track + i"
      ],
      "metadata": {
        "id": "kPFS5WRs45QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_lab, y_lab, random_state=42, train_size=0.7)"
      ],
      "metadata": {
        "id": "kAmKFSvOjWrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_temp = X_train_split + X_unlab\n",
        "y_train = y_train_split + y_unlab\n",
        "X_test_temp = X_test_split\n",
        "y_test = y_test_split\n",
        "\n",
        "sbert = SentenceTransformer('paraphrase-MiniLM-L3-v2')                  # https://www.sbert.net/docs/pretrained_models.html\n",
        "X_train = sbert.encode(X_train_temp)\n",
        "X_test = sbert.encode(X_test_temp)"
      ],
      "metadata": {
        "id": "b0YTJjzvta20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[:11815] -> 10000 unlab\n",
        "\n",
        "[:6815] -> 5000 unlab\n",
        "\n",
        "[:4815] -> 3000 unlab\n",
        "\n",
        "[:3630] -> 1815 unlab (1:1)\n",
        "\n",
        "[:2815] -> 1000 unlab\n",
        "\n",
        "[:2565] -> 750 unlab\n",
        "\n",
        "[:2315] -> 500 unlab\n",
        "\n",
        "[:2215] -> 400 unlab.   \n",
        "\n",
        "[:2115] -> 300 unlab\n",
        "\n",
        "[:2015] -> 200 unlab\n",
        "\n",
        "[:1815] -> 0 unlab"
      ],
      "metadata": {
        "id": "twv_-XRlG8Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train[:11815]      # [:2315] -> 500 unlab  [:2265] -> 450 unlab [:2215] -> 400 unlab.   [:2115] -> 300 unlab\n",
        "y_train = y_train[:11815]"
      ],
      "metadata": {
        "id": "l8CuqtSuohtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-aPJHL3P8m0",
        "outputId": "1faccef1-2070-498b-d067-9edd62d8e0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2315"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(dict(Counter(list(y_train))))\n",
        "# print(dict(Counter(list(y_test))))"
      ],
      "metadata": {
        "id": "RlYxO22cYouY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "r9jJYqxphxkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* triangle of inequality breached?\n",
        "* cos sim not a proper metric although natural for this case\n",
        "\n"
      ],
      "metadata": {
        "id": "lCfVesKeD3sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "## function tweaked from https://stackoverflow.com/questions/52057836/labelpropagation-how-to-avoid-division-by-zero\n",
        "def custom_rbf(X, Y=None, gamma=20):\n",
        "      # X, Y = metrics.pairwise.check_pairwise_arrays(X, Y)  \n",
        "      K = (1-metrics.pairwise.cosine_similarity(X, Y))**2\n",
        "      K *= -gamma \n",
        "      np.exp(K, K)\n",
        "      return K \n",
        "\n",
        "model = LabelPropagation(kernel = 'rbf', gamma = 0.2, max_iter = 500)\n",
        "# model = LabelPropagation(kernel = custom_rbf, tol = 0.1)   \n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "sc = model.score(X_test, y_test)\n",
        "print(sc)\n",
        "print(model.n_iter_)\n",
        "\n",
        "# model2 = LabelPropagation(kernel = 'rbf', gamma = 0.5, max_iter = 1000)\n",
        "# model2.fit(X_train, y_train)\n",
        "# sc = model2.score(X_test, y_test)\n",
        "# print(sc)\n",
        "# print(model2.n_iter_)"
      ],
      "metadata": {
        "id": "VVT6XYQyq8ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizations"
      ],
      "metadata": {
        "id": "eiipIzt-Icku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### from https://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_structure.html"
      ],
      "metadata": {
        "id": "phVnJMhgy4uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "# generate ring with inner box\n",
        "n_samples = 200\n",
        "X, y = make_circles(n_samples=n_samples, shuffle=False)\n",
        "outer, inner = 0, 1\n",
        "labels = np.full(n_samples, -1.0)\n",
        "labels[0] = outer\n",
        "labels[-1] = inner\n",
        "\n",
        "# #############################################################################\n",
        "# Learn with LabelSpreading\n",
        "label_spread = LabelSpreading(kernel=\"knn\", alpha=0.8)\n",
        "label_spread.fit(X, labels)\n",
        "\n",
        "# #############################################################################\n",
        "# Plot output labels\n",
        "output_labels = label_spread.transduction_\n",
        "plt.figure(figsize=(8.5, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(\n",
        "    X[labels == outer, 0],\n",
        "    X[labels == outer, 1],\n",
        "    color=\"navy\",\n",
        "    marker=\"s\",\n",
        "    lw=0,\n",
        "    label=\"outer labeled\",\n",
        "    s=10,\n",
        ")\n",
        "plt.scatter(\n",
        "    X[labels == inner, 0],\n",
        "    X[labels == inner, 1],\n",
        "    color=\"c\",\n",
        "    marker=\"s\",\n",
        "    lw=0,\n",
        "    label=\"inner labeled\",\n",
        "    s=10,\n",
        ")\n",
        "plt.scatter(\n",
        "    X[labels == -1, 0],\n",
        "    X[labels == -1, 1],\n",
        "    color=\"darkorange\",\n",
        "    marker=\".\",\n",
        "    label=\"unlabeled\",\n",
        ")\n",
        "plt.legend(scatterpoints=1, shadow=False, loc=\"upper right\")\n",
        "plt.title(\"Raw data (2 classes=outer and inner)\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "output_label_array = np.asarray(output_labels)\n",
        "outer_numbers = np.where(output_label_array == outer)[0]\n",
        "inner_numbers = np.where(output_label_array == inner)[0]\n",
        "plt.scatter(\n",
        "    X[outer_numbers, 0],\n",
        "    X[outer_numbers, 1],\n",
        "    color=\"navy\",\n",
        "    marker=\"s\",\n",
        "    lw=0,\n",
        "    s=10,\n",
        "    label=\"outer learned\",\n",
        ")\n",
        "plt.scatter(\n",
        "    X[inner_numbers, 0],\n",
        "    X[inner_numbers, 1],\n",
        "    color=\"c\",\n",
        "    marker=\"s\",\n",
        "    lw=0,\n",
        "    s=10,\n",
        "    label=\"inner learned\",\n",
        ")\n",
        "plt.legend(scatterpoints=1, shadow=False, loc=\"upper right\")\n",
        "plt.title(\"Labels learned with Label Spreading (KNN)\")\n",
        "\n",
        "plt.subplots_adjust(left=0.07, bottom=0.07, right=0.93, top=0.92)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XhNz7azryzIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distance Matrix"
      ],
      "metadata": {
        "id": "wJ1EUQRORlUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "guVy0SHdekwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from scipy.spatial import distance_matrix\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "data = 1 - cosine_similarity(X_test[50:100], X_test[50:100])\n",
        "ax = sns.heatmap(data, linewidth=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ygYJE4f14to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from scipy.spatial.distance import cosine\n",
        "# neigh = KNeighborsClassifier(weights='distance', algorithm='auto', metric = cosine)\n",
        "# neigh.fit(X_train[np.asarray(y_train)>-1], np.asarray(y_train)[np.asarray(y_train)>-1])\n",
        "# y_pred = neigh.predict(X_test)\n",
        "# print(neigh.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "hqCCkCgRHqK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### UMAP"
      ],
      "metadata": {
        "id": "IdjSjUmmf2tT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reducer = umap.UMAP(n_components=3)\n",
        "embedding = reducer.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "PouhPvPgHUMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot"
      ],
      "metadata": {
        "id": "p7yD_hw0McPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "u_labels = np.unique(y_pred)\n",
        "fig = plt.figure(figsize=(10,9))\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "for i in u_labels:\n",
        "    plt.scatter(embedding[y_pred == i , 0], embedding[y_pred == i , 1], embedding[y_pred == i , 2],  label = i)\n",
        "    # plt.scatter(X_test[y_pred == i , 0], X_test[y_pred == i , 1], X_test[y_pred == i , 2],  label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JV2rp5GtAEmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 classes (RS data) UNDER CONSTRUCTION"
      ],
      "metadata": {
        "id": "c1tRUglxkoJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# READSPEAKER SENTENCES\n",
        "angrysents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_02_anger.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    angrysents.append(sent)\n",
        "angrylabels = ['4' for i in range(len(angrysents))]\n",
        "\n",
        "enthusents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_03_happy.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    enthusents.append(sent)\n",
        "enthulabels = ['1' for i in range(len(enthusents))]\n",
        "\n",
        "sadsents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_04_sadness.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    sadsents.append(sent)\n",
        "sadlabels = ['3' for i in range(len(sadsents))]\n",
        "\n",
        "rs_sents = angrysents + enthusents + sadsents\n",
        "rs_labels = angrylabels + enthulabels + sadlabels"
      ],
      "metadata": {
        "id": "bW_0woUrjk0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_lab, X_unlab = [],[]\n",
        "y_lab, y_unlab = [],[]\n",
        "\n",
        "with open(\"thesis/corpora/eng-simple_wikipedia_2021_10K-sentences.txt\", encoding='utf-8') as f1:\n",
        "  f1 = (f1.read()).split(\"\\n\")\n",
        "  with open(\"thesis/corpora/eng-uk_web-public_2018_10K-sentences.txt\", encoding='utf-8') as f2:\n",
        "    f2 = (f2.read()).split(\"\\n\")\n",
        "    for i, line in enumerate(f1+f2):\n",
        "        sent = (line[(len(str(i+1))):]).strip()                                     # removes the line number and the tab char\n",
        "        if sent in trainsents:\n",
        "          X_lab.append(sent)\n",
        "          y_lab.append(combinedlabels[combinedsents.index(sent)])\n",
        "        else:\n",
        "          X_unlab.append(sent)\n",
        "          y_unlab.append(-1)\n",
        "\n",
        "for i, sent in enumerate(rs_sents):\n",
        "  X_lab.append(sent)\n",
        "  y_lab.append(combinedlabels[combinedsents.index(sent)])\n"
      ],
      "metadata": {
        "id": "p7oFTLmWnL3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_lab, y_lab, random_state=42, train_size=0.7)\n",
        "\n",
        "X_train_temp = X_train_split + X_unlab[:1815]\n",
        "y_train = [int(label) for label in y_train_split + y_unlab[:1815]]\n",
        "X_test_temp = X_test_split\n",
        "y_test = [int(label) for label in y_test_split]\n",
        "\n",
        "sbert = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "X_train = sbert.encode(X_train_temp)\n",
        "X_test = sbert.encode(X_test_temp)"
      ],
      "metadata": {
        "id": "C__KA1kKnWGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=42, train_size=0.7)\n",
        "neigh = KNeighborsClassifier(weights='distance', algorithm='auto', metric=cosine)      # weights='distance', algorithm='auto'\n",
        "neigh.fit(X_train, y_train)\n",
        "y_pred = neigh.predict(X_test)\n",
        "print(neigh.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "-7aCe5YKjk0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 classes positive-neutral-negative (RS data+neutralsents)"
      ],
      "metadata": {
        "id": "OLfZTMnqksQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary classification (2 classes)"
      ],
      "metadata": {
        "id": "pwZdMMSZksa3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary classification (2 classes) (comb happy+neut & ang+sad)"
      ],
      "metadata": {
        "id": "vdkIEVErkslK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[:6815] -> 5000 unlab\n",
        "\n",
        "[:4815] -> 3000 unlab\n",
        "\n",
        "[:3630] -> 1815 unlab (1:1)\n",
        "\n",
        "[:2815] -> 1000 unlab\n",
        "\n",
        "[:2565] -> 750 unlab\n",
        "\n",
        "[:2315] -> 500 unlab\n",
        "\n",
        "[:2265] -> 450 unlab \n",
        "\n",
        "[:2215] -> 400 unlab.   \n",
        "\n",
        "[:2115] -> 300 unlab\n",
        "\n",
        "[:1815] -> 0 unlab"
      ],
      "metadata": {
        "id": "r2CnYW-cm2Zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_temp = X_train_split + X_unlab\n",
        "y_train = y_train_split + y_unlab\n",
        "X_test_temp = X_test_split\n",
        "y_test = y_test_split\n",
        "\n",
        "sbert = SentenceTransformer('paraphrase-MiniLM-L3-v2')                  # https://www.sbert.net/docs/pretrained_models.html\n",
        "X_train = sbert.encode(X_train_temp)\n",
        "X_test = sbert.encode(X_test_temp)"
      ],
      "metadata": {
        "id": "5AtzNI4BnRsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tX_train = X_train[:6815]\n",
        "y_train = y_train[:6815]"
      ],
      "metadata": {
        "id": "5srPc_eKm2Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.semi_supervised import LabelSpreading\n",
        "model = LabelSpreading(kernel = 'rbf', gamma = 0.3, max_iter = 500)\n",
        "#(kernel = 'rbf', gamma = 1.0, max_iter = 5000)  \n",
        "#(kernel = rbf_kernel_safe, tol = 0.01, gamma = 0.001)   \n",
        "#(kernel = 'rbf', gamma = 6.0, max_iter = 1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "sc = model.score(X_test, y_test)\n",
        "print(sc)"
      ],
      "metadata": {
        "id": "sStiTbgH4S7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_hyperparameters = None\n",
        "grid = {'kernel': ['rbf'],\n",
        "        'gamma': [0.5, 0.2, 0.1, 0.01, 0.005, 0.7, 0.6],\n",
        "        'max_iter': [200, 300, 400,100]}\n",
        "print(\"kernel:\\tgamma:\\tn_neighbors:\\tmax_iter:\\tTraining set accuracy:\")\n",
        "\n",
        "for i in range(20):\n",
        "  kn = grid['kernel'][0]\n",
        "  gm = grid['gamma'][random.randint(0,6)]\n",
        "  maxit = grid['max_iter'][random.randint(0,3)]\n",
        "  # print(kn, gm, maxit)\n",
        "  \n",
        "  model = LabelSpreading(kernel = kn, gamma = gm, max_iter=maxit)\n",
        "  model.fit(X_train, y_train)\n",
        "  # y_pred = model.predict(X_test)\n",
        "  training_accuracy = model.score(X_test, y_test)\n",
        "  if best_hyperparameters is None or best_hyperparameters[3] < training_accuracy:\n",
        "    best_hyperparameters = (kn, gm, maxit, training_accuracy)\n",
        "  print(f\"{kn}\\t\\t{gm}\\t\\t{maxit}\\t\\t{training_accuracy}\")\n",
        "\n",
        "best_kernel = best_hyperparameters[0]\n",
        "best_gamma = best_hyperparameters[1]\n",
        "best_maxit = best_hyperparameters[2]\n",
        "best_score = best_hyperparameters[3]\n",
        "\n",
        "print(f\"Best parameters:{best_kernel}\\t{best_gamma}\\t{best_maxit}\\t{best_score}\")"
      ],
      "metadata": {
        "id": "OWnQUPWSCR4-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}