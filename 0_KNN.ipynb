{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0-KNN",
      "provenance": [],
      "collapsed_sections": [
        "fRgWH25wef4H",
        "FsBF2lVN0Xtz",
        "6RcfmRBjMzcW",
        "cheznZfnM36F",
        "IFA3xVkaJjqL",
        "p7yD_hw0McPI",
        "GUVj32SQiUjp",
        "UXX1HwY7lCV3",
        "xgtNH8jLilh7",
        "AOB48j0Kjk0Z"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP5n67YsQnMAEgFdzIxFEuy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irenetsk/mastersthesis/blob/main/0_KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#K-Neighbours Classification\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier"
      ],
      "metadata": {
        "id": "-VcFBrsjMRAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/My Drive/\n",
        "! pip install umap-learn\n",
        "! pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "zoz315Ga4_11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN with all 4 classes (**Fine-grained** Classification)"
      ],
      "metadata": {
        "id": "fRgWH25wef4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Set Creation\n",
        "1 = Enthusiastic / 2 = Neutral / 3 = Sad / 4 = Angry\n"
      ],
      "metadata": {
        "id": "6RcfmRBjMzcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# READSPEAKER SENTENCES\n",
        "angrysents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_02_anger.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    angrysents.append(sent)\n",
        "angrylabels = [4 for i in range(len(angrysents))]\n",
        "\n",
        "enthusents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_03_happy.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    enthusents.append(sent)\n",
        "enthulabels = [1 for i in range(len(enthusents))]\n",
        "\n",
        "sadsents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_04_sadness.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    sadsents.append(sent)\n",
        "sadlabels = [3 for i in range(len(sadsents))]\n",
        "\n",
        "# MY ANNOTATIONS\n",
        "trainsents = []\n",
        "with open(\"./thesis/myannotations/sents.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    trainsents.append(sent)\n",
        "\n",
        "trainlabels = []\n",
        "with open(\"./thesis/myannotations/labels.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for label in f:\n",
        "    trainlabels.append(label)\n",
        "\n",
        "with open(\"./thesis/myannotations/moreneutral_sents.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    trainsents.append(sent)\n",
        "  for i in range(len(f)):\n",
        "    trainlabels.append('2')\n",
        "\n",
        "# rs_sents = angrysents + enthusents + sadsents\n",
        "# rs_labels = angrylabels + enthulabels + sadlabels\n",
        "comb_sents = trainsents + angrysents + enthusents + sadsents\n",
        "comb_labels = trainlabels + angrylabels + enthulabels + sadlabels"
      ],
      "metadata": {
        "id": "k3OtDSvmvnsI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f'enthusiastic: \\t{len(enthusents)} \\nsad: \\t\\t{len(sadsents)}\\nangry: \\t\\t{len(angrysents)}\\nmy annotations: {len(trainsents)}\\ntotal: \\t\\t{sum([len(angrysents), len(enthusents), len(sadsents), len(trainsents)])}')"
      ],
      "metadata": {
        "id": "sARtP_0LOG7t"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainsentsind = dict()\n",
        "allsentences = list()\n",
        "\n",
        "# with open(\"thesis/corpora/eng-simple_wikipedia_2021_10K-sentences.txt\", encoding='utf-8') as f1:\n",
        "#   f1 = (f1.read()).split(\"\\n\")\n",
        "#   with open(\"thesis/corpora/eng-uk_web-public_2018_10K-sentences.txt\", encoding='utf-8') as f2:\n",
        "#     f2 = (f2.read()).split(\"\\n\")\n",
        "#     for i, line in enumerate(f1+f2):\n",
        "#         sent = (line[(len(str(i+1))):]).strip()             #removes the line number and the tab char\n",
        "#         if sent in trainsents:\n",
        "#           trainsentsind[sent] = i\n",
        "#         allsentences.append(sent)\n",
        "\n",
        "# track = len(allsentences)\n",
        "for i, sent in enumerate(comb_sents):\n",
        "  allsentences.append(sent)\n",
        "  # trainsentsind[sent] = track + i\n",
        "\n",
        "embeddings = model.encode(allsentences)\n",
        "# embeddings = model.encode(allsentences)"
      ],
      "metadata": {
        "id": "kPFS5WRs45QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(allsentences)"
      ],
      "metadata": {
        "id": "7dx3SdtavcMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "C-V2HgbAksk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([embeddings[i] for i, sent in enumerate(comb_sents)])\n",
        "y_train = [int(element) for element in comb_labels]"
      ],
      "metadata": {
        "id": "Xs5zSXloz3T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import Counter\n",
        "# a = dict(Counter(alltrainlabels))\n",
        "# a"
      ],
      "metadata": {
        "id": "S10KxVTuYI_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Split"
      ],
      "metadata": {
        "id": "cheznZfnM36F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=42, train_size=0.65)"
      ],
      "metadata": {
        "id": "DDMeMPYPSlng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid search & Run"
      ],
      "metadata": {
        "id": "IFA3xVkaJjqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'weights': ('uniform', 'distance'), 'algorithm':('auto', 'ball_tree', 'kd_tree', 'brute')}\n",
        "neigh = KNeighborsClassifier()\n",
        "clf = GridSearchCV(neigh, parameters)\n",
        "clf.fit(X_train, y_train)\n",
        "sorted(clf.cv_results_.keys())"
      ],
      "metadata": {
        "id": "8TahLczTh85r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import random\n",
        "# best_hyperparameters = None\n",
        "# grid = {'weights': ['uniform', 'distance'],\n",
        "#         'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
        "# print(\"Weights:\\tAlgorithm:\\tTraining set accuracy:\")\n",
        "# for i in range(20):\n",
        "#   wt = grid['weights'][random.randint(0,1)]\n",
        "#   al = grid['algorithm'][random.randint(0,3)]\n",
        "#   model = KNeighborsClassifier(weights = wt, algorithm = al)\n",
        "#   model.fit(X_train, y_train)\n",
        "#   training_accuracy = model.score(X_test, y_test)\n",
        "#   if best_hyperparameters is None or best_hyperparameters[2] < training_accuracy:\n",
        "#     best_hyperparameters = (wt, al, training_accuracy)\n",
        "#   print(f\"{wt}\\t\\t{al}\\t\\t{training_accuracy}\")\n",
        "# best_weights = best_hyperparameters[0]\n",
        "# best_algorithm = best_hyperparameters[1]\n",
        "# print(f\"Best parameters:{best_weights}\\t{best_algorithm}\")"
      ],
      "metadata": {
        "id": "9_NyPIwnS6C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "neigh = KNeighborsClassifier(weights='distance', algorithm='auto')\n",
        "neigh.fit(X_train, y_train)\n",
        "y_pred = neigh.predict(X_test)\n",
        "print(neigh.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "Ewqevz2y6u70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADD UMAP HERE FOR VISUALIZATION"
      ],
      "metadata": {
        "id": "IdjSjUmmf2tT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ],
      "metadata": {
        "id": "p7yD_hw0McPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "u_labels = np.unique(y_pred)\n",
        "fig = plt.figure(figsize=(10,9))\n",
        "# ax = plt.axes(projection='3d')\n",
        "\n",
        "for i in u_labels:\n",
        "    plt.scatter(X_test[y_pred == i , 0], X_test[y_pred == i , 1], X_test[y_pred == i , 2],  label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JV2rp5GtAEmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training/testing with only 3 classes (RS data)\n",
        "Score: **0.634011090573013**"
      ],
      "metadata": {
        "id": "GUVj32SQiUjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "angrysents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_02_anger.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:    #[:-478]:\n",
        "    angrysents.append(sent)\n",
        "angrylabels = [4 for i in range(len(angrysents))]\n",
        "\n",
        "enthusents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_03_happy.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f[:-50]:\n",
        "    enthusents.append(sent)\n",
        "enthulabels = [1 for i in range(len(enthusents))]\n",
        "\n",
        "sadsents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_04_sadness.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f[:-38]:\n",
        "    sadsents.append(sent)\n",
        "sadlabels = [3 for i in range(len(sadsents))]\n",
        "\n",
        "X = angrysents + enthusents + sadsents\n",
        "y = angrylabels + enthulabels + sadlabels\n",
        "\n",
        "embeddings = model.encode(X)\n",
        "reducer = umap.UMAP(n_components=3)\n",
        "embedding = reducer.fit_transform(embeddings)\n",
        "\n",
        "X_train = np.array([embedding[i] for i, sent in enumerate(X)])\n",
        "y_train = y"
      ],
      "metadata": {
        "id": "AvoCFVmJdDCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vec2sent = dict()\n",
        "# for i, sentence in enumerate(X):\n",
        "#   vec2sent[str(embedding[i])] = sentence"
      ],
      "metadata": {
        "id": "MsniKkRvfV82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=42, train_size=0.7)\n",
        "neigh = KNeighborsClassifier(weights='distance', algorithm='auto')      # weights='distance', algorithm='auto'\n",
        "neigh.fit(X_train, y_train)\n",
        "y_pred = neigh.predict(X_test)\n",
        "print(neigh.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "mO8Cs2Zwd_cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training/testing with 3 classes positive-neutral-negative (RS data+neutralsents)\n",
        "Score: **0.8062730627306273**"
      ],
      "metadata": {
        "id": "UXX1HwY7lCV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "enthusents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_03_happy.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f[:-50]:\n",
        "    enthusents.append(sent)\n",
        "enthulabels = [0 for i in range(len(enthusents))]\n",
        "\n",
        "sadsents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_04_sadness.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f[:-38]:\n",
        "    sadsents.append(sent)\n",
        "sadlabels = [2 for i in range(len(sadsents))]\n",
        "\n",
        "neutralsents = []\n",
        "with open(\"./thesis/myannotations/moreneutral_sents.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    neutralsents.append(sent)\n",
        "neutrallabels = [1 for i in range(len(neutralsents))]\n",
        "\n",
        "X = enthusents + neutralsents + sadsents\n",
        "y = enthulabels + neutrallabels + sadlabels\n",
        "\n",
        "embeddings = model.encode(X)\n",
        "reducer = umap.UMAP(n_components=3)\n",
        "embedding = reducer.fit_transform(embeddings)\n",
        "\n",
        "X_train = np.array([embedding[i] for i, sent in enumerate(X)])\n",
        "y_train = y"
      ],
      "metadata": {
        "id": "praSZwG0lEsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=42, train_size=0.7)\n",
        "neigh = KNeighborsClassifier(weights='distance', algorithm='auto')      # weights='distance', algorithm='auto'\n",
        "neigh.fit(X_train, y_train)\n",
        "y_pred = neigh.predict(X_test)\n",
        "print(neigh.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "eav2acIwlcE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/test with 2 classes (**binary** classification)\n",
        "Score: **0.847457627118644**"
      ],
      "metadata": {
        "id": "xgtNH8jLilh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enthusents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_03_happy.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f[:-50]:\n",
        "    enthusents.append(sent)\n",
        "enthulabels = [1 for i in range(len(enthusents))]\n",
        "\n",
        "sadsents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_04_sadness.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f[:-38]:\n",
        "    sadsents.append(sent)\n",
        "sadlabels = [3 for i in range(len(sadsents))]\n",
        "\n",
        "X = enthusents + sadsents\n",
        "y = enthulabels + sadlabels\n",
        "\n",
        "embeddings = model.encode(X)\n",
        "reducer = umap.UMAP(n_components=3)\n",
        "embedding = reducer.fit_transform(embeddings)\n",
        "\n",
        "X_train = np.array([embedding[i] for i, sent in enumerate(X)])\n",
        "y_train = y"
      ],
      "metadata": {
        "id": "-Bv5Qw22insO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=42, train_size=0.7)\n",
        "neigh = KNeighborsClassifier(weights='distance', algorithm='auto')      # weights='distance', algorithm='auto'\n",
        "neigh.fit(X_train, y_train)\n",
        "y_pred = neigh.predict(X_test)\n",
        "print(neigh.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "zAG1BTOvinsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/test with 2 classes (comb happy+neut & ang+sad)\n",
        "Score: **0.8047091412742382**"
      ],
      "metadata": {
        "id": "AOB48j0Kjk0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "angrysents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_02_anger.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:    #[:-478]:\n",
        "    angrysents.append(sent)\n",
        "angrylabels = [2 for i in range(len(angrysents))]\n",
        "\n",
        "enthusents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_03_happy.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f[:-50]:\n",
        "    enthusents.append(sent)\n",
        "enthulabels = [1 for i in range(len(enthusents))]\n",
        "\n",
        "sadsents = []\n",
        "with open(\"./thesis/readspeaker_annots/emotion_04_sadness.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f[:-38]:\n",
        "    sadsents.append(sent)\n",
        "sadlabels = [2 for i in range(len(sadsents))]\n",
        "\n",
        "neutralsents = []\n",
        "with open(\"./thesis/myannotations/moreneutral_sents.txt\", encoding='utf-8') as f:\n",
        "  f = (f.read()).split(\"\\n\")\n",
        "  for sent in f:\n",
        "    neutralsents.append(sent)\n",
        "neutrallabels = [1 for i in range(len(neutralsents))]\n",
        "\n",
        "\n",
        "\n",
        "X = enthusents + neutralsents + angrysents + sadsents\n",
        "y = enthulabels + neutrallabels + sadlabels + angrylabels\n",
        "\n",
        "embeddings = model.encode(X)\n",
        "reducer = umap.UMAP(n_components=3)\n",
        "embedding = reducer.fit_transform(embeddings)\n",
        "\n",
        "X_train = np.array([embedding[i] for i, sent in enumerate(X)])\n",
        "y_train = y"
      ],
      "metadata": {
        "id": "bW_0woUrjk0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=42, train_size=0.7)\n",
        "neigh = KNeighborsClassifier(weights='distance', algorithm='auto')      # weights='distance', algorithm='auto'\n",
        "neigh.fit(X_train, y_train)\n",
        "y_pred = neigh.predict(X_test)\n",
        "print(neigh.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "-7aCe5YKjk0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}